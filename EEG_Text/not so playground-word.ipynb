{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8e3d42-96db-4470-8b0c-1724f5d15dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b738a3-05fb-4ad1-9421-cfda1abb2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mamba_ssm import Mamba\n",
    "# from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "import scipy as sp\n",
    "\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from mamba_ssm.modules.block import Block\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from mamba_model import MambaEEG\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "from models import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a939243-7b29-462c-93d7-54f1ea934d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50b8817-e077-4494-8bb6-3e9af187aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matlab_string(matlab_extracted_object):\n",
    "    \"\"\"\n",
    "    Converts a string loaded from h5py into a python string\n",
    "    :param matlab_extracted_object:     (h5py)  matlab string object\n",
    "    :return:\n",
    "        extracted_string    (str)   translated string\n",
    "    \"\"\"\n",
    "\n",
    "    # print((chr(c) for c in matlab_extracted_object))\n",
    "    extracted_string = u''.join(chr(c) for c in matlab_extracted_object[:].flatten())\n",
    "    # print(extracted_string)\n",
    "    return extracted_string\n",
    "\n",
    "\n",
    "def is_real_word(word):\n",
    "    \"\"\"\n",
    "    Check if the word is a real word\n",
    "    :param word:    (str)   word string\n",
    "    :return:\n",
    "        is_word (bool)  True if it is a real word\n",
    "    \"\"\"\n",
    "    is_word = re.search('[a-zA-Z0-9]', word)\n",
    "    return is_word\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9152608e-953c-4668-ae81-7cf5df84310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = \"NR\"\n",
    "\n",
    "# rootdir = \"/radraid/spanchavati/eegtotext/zuco-benchmark/data/\"\n",
    "\n",
    "# print('##############################')\n",
    "# print(f'start processing ZuCo task2-NR-2.0...')\n",
    "\n",
    "# dataset_dict = {}\n",
    "\n",
    "# for file in tqdm(os.listdir(rootdir)[::-1]):\n",
    "#     if file.endswith(task+\".mat\"):\n",
    "#         print(file)\n",
    "\n",
    "#         file_name = rootdir + file\n",
    "\n",
    "#         # print('file name:', file_name)\n",
    "#         subject = file_name.split(\"ts\")[1].split(\"_\")[0]\n",
    "#         # print('subject: ', subject)\n",
    "\n",
    "#         # exclude YMH due to incomplete data because of dyslexia\n",
    "#         if subject != 'YMH':\n",
    "#             pass\n",
    "\n",
    "#         f = h5py.File(file_name,'r')\n",
    "#         print('keys in f:', list(f.keys()))\n",
    "#         try:\n",
    "#             sentence_data = f['sentenceData']\n",
    "#             # break\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "#         contents = []\n",
    "#         rawEEG = []\n",
    "#         for i in range(sentence_data['rawData'].len()):\n",
    "#             content = load_matlab_string(f[sentence_data['content'][i][0]])\n",
    "#             raweeg = f[sentence_data['rawData'][i][0]]\n",
    "\n",
    "#             contents.append(content)\n",
    "#             rawEEG.append(np.array(raweeg))\n",
    "\n",
    "#         dataset_dict[subject] = {'content': contents, 'eeg': rawEEG}\n",
    "#         #     # contents.append(sentence_data['content'])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e1bb23-c980-4bfe-922a-63261ab71918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import numpy as np\n",
    "\n",
    "# # Recursive function to extract data from nested h5py objects\n",
    "# def recursively_extract_data(name, obj):\n",
    "#     if isinstance(obj, h5py.Dataset):\n",
    "#         return obj[()]\n",
    "#     elif isinstance(obj, h5py.Group):\n",
    "#         return {key: recursively_extract_data(key, obj[key]) for key in obj.keys()}\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Function to load .mat files\n",
    "# def load_mat_file(file_path):\n",
    "#     with h5py.File(file_path, 'r') as f:\n",
    "#         data = recursively_extract_data('/', f)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c886a-d4bf-4218-a242-268d277a675e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0fb626-a1ed-4c08-b4de-5a269df0df06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "start processing ZuCo task2-NR-2.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47941cacbd74d009d3e912bf3abf854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultsYAG_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsXAH_NR.mat\n",
      "keys in f: []\n",
      "resultsXLS_NR.mat\n",
      "keys in f: []\n",
      "resultsYRK_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsYDR_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsYRP_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsYFR_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsYHS_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsXDT_NR.mat\n",
      "keys in f: []\n",
      "resultsXBB_NR.mat\n",
      "keys in f: []\n",
      "resultsYSL_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsYTL_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsXWS_NR.mat\n",
      "keys in f: []\n",
      "resultsYIS_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsYMD_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsXBD_NR.mat\n",
      "keys in f: []\n",
      "resultsYFS_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsXSS_NR.mat\n",
      "keys in f: []\n",
      "resultsYSD_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsYLS_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsXPB_NR.mat\n",
      "keys in f: []\n",
      "resultsXTR_NR.mat\n",
      "keys in f: []\n",
      "resultsXSE_NR.mat\n",
      "keys in f: []\n",
      "resultsYDG_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsYAC_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n",
      "resultsYAK_NR.mat\n",
      "keys in f: ['#refs#', 'sentenceData']\n"
     ]
    }
   ],
   "source": [
    "task = \"NR\"\n",
    "\n",
    "rootdir = \"/radraid/spanchavati/eegtotext/zuco-benchmark/data/\"\n",
    "\n",
    "print('##############################')\n",
    "print(f'start processing ZuCo task2-NR-2.0...')\n",
    "\n",
    "dataset_dict = {}\n",
    "\n",
    "all_words = set()\n",
    "\n",
    "for file in tqdm(os.listdir(rootdir)[::-1]):\n",
    "    if file.endswith(task+\".mat\"):\n",
    "        print(file)\n",
    "\n",
    "        file_name = rootdir + file\n",
    "\n",
    "        # print('file name:', file_name)\n",
    "        subject = file_name.split(\"ts\")[1].split(\"_\")[0]\n",
    "        # print('subject: ', subject)\n",
    "\n",
    "        # exclude YMH due to incomplete data because of dyslexia\n",
    "        if subject != 'YMH':\n",
    "            pass\n",
    "\n",
    "        f = h5py.File(file_name,'r')\n",
    "        print('keys in f:', list(f.keys()))\n",
    "        try:\n",
    "            sentence_data = f['sentenceData']\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        contents = []\n",
    "        rawEEG = []\n",
    "        for i in range(sentence_data['word'].len()):\n",
    "            sentence = sentence_data['word'][i][0]\n",
    "            sentence = f[sentence]\n",
    "\n",
    "            \n",
    "            if isinstance(sentence, h5py.Dataset) or 'rawEEG' not in sentence.keys():\n",
    "                continue\n",
    "            for j in range(sentence['rawEEG'].len()):\n",
    "                content = f[sentence['content'][j][0]]\n",
    "                content = load_matlab_string(content)\n",
    "                content = content.replace(',','')\n",
    "                content = content.replace('.','')\n",
    "                \n",
    "\n",
    "                if not is_real_word(content) or len(re.findall(r'[A-z]',content)) == 0:\n",
    "                    continue\n",
    "                \n",
    "\n",
    "                try:\n",
    "                    raweeg = np.array(f[f[sentence['rawEEG'][j][0]][0][0]])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                contents.append(content)\n",
    "                rawEEG.append(raweeg)\n",
    "\n",
    "            \n",
    "        all_words.update(contents)\n",
    "        dataset_dict[subject] = {'content': contents, 'eeg': rawEEG}\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc45aaa5-11f4-4aeb-9546-d7b554f08e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for d in dataset_dict:\n",
    "    lens.extend([p.shape[0] for p in dataset_dict[d]['eeg']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac92a9a9-495b-49eb-ac3f-e9eb7477155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EEGTextDatasetV2(Dataset):\n",
    "    def __init__(self, data_dict, subject_keys, tokenizer_name='bert-base-uncased', maxlen=15*500, mode='within'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.maxlen = maxlen\n",
    "        self.data = []\n",
    "        self.subject_to_id = {}\n",
    "        self.mode = mode  # 'within', 'cross', or 'zero-shot'\n",
    "        \n",
    "        self.load_data(data_dict, subject_keys)\n",
    "\n",
    "    def load_data(self, data_dict, subject_keys):\n",
    "        for i, key in enumerate(subject_keys):\n",
    "            patient_data = data_dict[key]\n",
    "            sentences = np.array(patient_data['content'])\n",
    "            eeg_data = patient_data['eeg']\n",
    "            \n",
    "            if key not in self.subject_to_id:\n",
    "                self.subject_to_id[key] = len(self.subject_to_id)\n",
    "            subject_id = self.subject_to_id[key]\n",
    "            \n",
    "            mean, std = self.incremental_mean_std(eeg_data)\n",
    "\n",
    "            for sentence, eeg in zip(sentences, eeg_data):\n",
    "                eeg_processed, attention_mask = self.process_eeg(eeg, mean, std)\n",
    "                if eeg_processed is not None:\n",
    "                    self.data.append({\n",
    "                        'sentence': sentence,\n",
    "                        'eeg': eeg_processed,\n",
    "                        'eeg_attention_mask': attention_mask,\n",
    "                        'subject_id': subject_id\n",
    "                    })\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        tokenized = self.tokenizer(item['sentence'], return_tensors='pt', padding='max_length', truncation=True)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'][0],\n",
    "            'attention_mask': tokenized['attention_mask'][0],\n",
    "            'eeg': torch.nan_to_num(torch.tensor(item['eeg']), posinf=0, neginf=0).float(),\n",
    "            'eeg_attention_mask': torch.tensor(item['eeg_attention_mask']),\n",
    "            'subject_id': torch.tensor(item['subject_id'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_eeg(self, eeg_data, mean, std):\n",
    "        \"\"\"\n",
    "        Normalize EEG by computing total channel mean and std.\n",
    "        Right pad EEG with 0s to self.maxlen, throw error if eeg_data is longer than maxlen.\n",
    "        \"\"\"\n",
    "        if eeg_data.shape[0] < 100:\n",
    "            return None, None\n",
    "    \n",
    "        normalized_eeg = (eeg_data - mean) / std\n",
    "        \n",
    "        # Check if EEG data length exceeds maxlen\n",
    "        if normalized_eeg.shape[0] > self.maxlen:\n",
    "            print(f\"EEG data length {normalized_eeg.shape[0]} exceeds maxlen {self.maxlen}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = np.zeros((self.maxlen,))\n",
    "        attention_mask[:normalized_eeg.shape[0]] = 1\n",
    "        \n",
    "        # Right pad EEG data with zeros\n",
    "        padded_eeg = np.zeros((self.maxlen, normalized_eeg.shape[1]))\n",
    "        padded_eeg[:normalized_eeg.shape[0], :] = normalized_eeg\n",
    "        \n",
    "        return padded_eeg, attention_mask\n",
    "    \n",
    "    def incremental_mean_std(self, data_list):\n",
    "        \"\"\"\n",
    "        Calculate mean and standard deviation incrementally for a list of EEG data arrays.\n",
    "        \"\"\"\n",
    "        n_total = 0\n",
    "        mean = 0\n",
    "        M2 = 0\n",
    "        for data in data_list:\n",
    "            n = data.shape[0]\n",
    "            if n < 100:\n",
    "                continue\n",
    "        n_total += n\n",
    "        delta = data - mean\n",
    "        mean += np.nansum(delta, axis=0) / n_total\n",
    "        delta2 = data - mean\n",
    "        M2 += np.nansum(delta * delta2, axis=0)\n",
    "\n",
    "        variance = M2 / (n_total - 1)\n",
    "        std = np.sqrt(variance)\n",
    "        return mean, std\n",
    "\n",
    "def create_data_splits(data_dict, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    all_sentences = set()\n",
    "    for subject_data in data_dict.values():\n",
    "        all_sentences.update(subject_data['content'])\n",
    "    \n",
    "    # Select test (zero-shot) sentences\n",
    "    test_sentences = set(random.sample(all_sentences, int(len(all_sentences) * test_ratio)))\n",
    "    \n",
    "    train_val_data = {subject: {'content': [], 'eeg': []} for subject in data_dict}\n",
    "    test_data = {subject: {'content': [], 'eeg': []} for subject in data_dict}\n",
    "\n",
    "    for subject, subject_data in data_dict.items():\n",
    "        for sentence, eeg in zip(subject_data['content'], subject_data['eeg']):\n",
    "            if sentence in test_sentences:\n",
    "                test_data[subject]['content'].append(sentence)\n",
    "                test_data[subject]['eeg'].append(eeg)\n",
    "                # print(eeg)\n",
    "            else:\n",
    "                train_val_data[subject]['content'].append(sentence)\n",
    "                train_val_data[subject]['eeg'].append(eeg)\n",
    "    \n",
    "    # Split remaining data into train and validation\n",
    "    train_data = {subject: {'content': [], 'eeg': []} for subject in data_dict}\n",
    "    val_data = {subject: {'content': [], 'eeg': []} for subject in data_dict}\n",
    "\n",
    "    for subject, subject_data in train_val_data.items():\n",
    "        n = len(subject_data['content'])\n",
    "        train_idx = int(n * (train_ratio / (train_ratio + val_ratio)))\n",
    "        \n",
    "        train_data[subject]['content'] = subject_data['content'][:train_idx]\n",
    "        train_data[subject]['eeg'] = subject_data['eeg'][:train_idx]\n",
    "        \n",
    "        val_data[subject]['content'] = subject_data['content'][train_idx:]\n",
    "        val_data[subject]['eeg'] = subject_data['eeg'][train_idx:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def create_datasets(data_dict, tokenizer_name, maxlen):\n",
    "    train_data, val_data, test_data = create_data_splits(data_dict)\n",
    "    \n",
    "    # # Within-subject datasets\n",
    "    # train_within = EEGTextDatasetV2(train_data, list(train_data.keys()), tokenizer_name, maxlen, mode='within')\n",
    "    # val_within = EEGTextDatasetV2(val_data, list(val_data.keys()), tokenizer_name, maxlen, mode='within')\n",
    "    \n",
    "    # Cross-subject dataset\n",
    "    train_data = {subject: {'content': train_data[subject]['content'],\n",
    "                          'eeg': train_data[subject]['eeg']}\n",
    "                for subject in train_data.keys()}\n",
    "    val_data = {subject: {'content': val_data[subject]['content'],\n",
    "                          'eeg':val_data[subject]['eeg']} for subject in val_data.keys()}\n",
    "    \n",
    "    train_cross = EEGTextDatasetV2(train_data, list(train_data.keys()), tokenizer_name, maxlen, mode='cross')\n",
    "    val_cross = EEGTextDatasetV2(val_data, list(val_data.keys()), tokenizer_name, maxlen, mode='cross')\n",
    "    \n",
    "    # Test dataset (zero-shot)\n",
    "    test = EEGTextDatasetV2(test_data, list(test_data.keys()), tokenizer_name, maxlen, mode='test')\n",
    "    \n",
    "    return train_cross, val_cross, test # train_within, val_within, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f189582-020a-4070-95f1-9df67e058d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_name = model_name\n",
    "maxlen = 2*500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1adf4c82-5723-4c32-820e-142119384e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1131483/1136260562.py:97: RuntimeWarning: invalid value encountered in true_divide\n",
      "  variance = M2 / (n_total - 1)\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = create_datasets(dataset_dict, tokenizer_name, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0427742-d497-4125-a530-b5064cbd0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True,num_workers= 4)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_ds, batch_size = 64, shuffle = False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f24986-5430-4611-818a-42a104a723b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # del model\n",
    "# # del batch\n",
    "\n",
    "# # import gc\n",
    "# # gc.collect()\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00282a83-0f47-4959-8202-690566411ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d3cefc7-8f64-4226-a084-229e834e455d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/spanchavati/anaconda3/envs/eegtext/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'eeg_encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['eeg_encoder'])`.\n",
      "  rank_zero_warn(\n",
      "/raid/spanchavati/anaconda3/envs/eegtext/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'text_encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['text_encoder'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "encoder = HuggingFaceEncoder(model_name, freeze = True)\n",
    "\n",
    "\n",
    "mm = MambaConfig(ssm_cfg = {'layer':'Mamba1'}, d_model = 64, n_layer = 12)\n",
    "\n",
    "ee = EEGEncoder(n_channels = 105, max_length= 2*500, mamba_config=mm, embedding = 'mean', patient_ids = list(train_ds.subject_to_id.values()))\n",
    "\n",
    "\n",
    "\n",
    "model = EEGTextCLIP(\n",
    "    eeg_encoder=ee,\n",
    "    text_encoder=encoder,\n",
    "    text_embedding_dims=768,\n",
    "    projection_dims=128,\n",
    "    dropout=0.1,\n",
    "    temperature=1.0,\n",
    "    weight_decay=1e-5,\n",
    "    head_lr=1e-3,\n",
    "    image_encoder_lr=1e-3,\n",
    "    text_encoder_lr=1e-4,\n",
    "    lr_scheduler_patience=5.0,\n",
    "    lr_scheduler_factor=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30da3d-66d2-4b03-b116-4e556d37dbe3",
   "metadata": {},
   "source": [
    "<!-- # for batch in train_dataloader:\n",
    "#     break -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e6c2ab-571f-42e9-be1d-ae1fa41dd68f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4354085-1ac9-495b-8c1d-4ce091aa4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del batch\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ea9a4e7-9d75-4a70-bf66-2806d96ab1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8ba30-bf1c-4440-98f0-07c38868ed20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd48eef37ff24694b7a0f450791fe11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c43501c3b95486b992846b123efd7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Initialize tensorboard writer\n",
    "writer = SummaryWriter(filename_suffix = 'wordlevel')\n",
    "\n",
    "# Get optimizer and scheduler\n",
    "optim_config = model.configure_optimizers()\n",
    "optimizer = optim_config['optimizer']\n",
    "lr_scheduler = optim_config['lr_scheduler']\n",
    "\n",
    "num_epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Create a tqdm progress bar for epochs\n",
    "epoch_bar = tqdm(range(num_epochs), desc=\"Training\", position=0)\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Create a tqdm progress bar for batches\n",
    "    batch_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch}\", position=1, leave=False)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(batch_bar):\n",
    "        batch = {b: batch[b].to(device) for b in batch}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        eeg_embeddings, text_embeddings = model(batch)\n",
    "        loss = model._compute_losses(eeg_embeddings, text_embeddings).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update batch progress bar\n",
    "        batch_bar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        })\n",
    "        \n",
    "        # Log training loss\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * len(train_dataloader) + batch_idx)\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = {b: batch[b].to(device) for b in batch}\n",
    "            eeg_embeddings, text_embeddings = model(batch)\n",
    "            loss = model._compute_losses(eeg_embeddings, text_embeddings).mean()\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    \n",
    "    # Log validation loss\n",
    "    writer.add_scalar('Loss/val', avg_val_loss, epoch)\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    lr_scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Log learning rate\n",
    "    writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    # Update epoch progress bar\n",
    "    epoch_bar.set_postfix({\n",
    "        'train_loss': f\"{avg_train_loss:.4f}\",\n",
    "        'val_loss': f\"{avg_val_loss:.4f}\",\n",
    "        'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\",\n",
    "        'best_val_loss': f\"{best_val_loss:.4f}\"\n",
    "    })\n",
    "\n",
    "# Close tensorboard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5169c-9c19-48af-849f-1aa9478dd985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d471a7-e8ea-4939-9cb3-d92158720e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6a8f5-1d0a-487b-98c2-7101deb26b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_sentences(data_dict):\n",
    "    unique_sentences = {}\n",
    "    for key, patient_data in data_dict.items():\n",
    "        for sentence in patient_data['content']:\n",
    "            if sentence not in unique_sentences:\n",
    "                unique_sentences[sentence] = len(unique_sentences)\n",
    "    return unique_sentences\n",
    "\n",
    "def embed_unique_sentences(model, unique_sentences, tokenizer):\n",
    "    model.eval()\n",
    "    sentence_order = list(unique_sentences)\n",
    "    inputs = tokenizer(sentence_order, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(inputs.to(device))\n",
    "        text_embeddings = model.text_proj(text_features.to(device))\n",
    "    return text_embeddings, sentence_order\n",
    "\n",
    "def embed_eeg_data(model, dataloader):\n",
    "    model.eval()\n",
    "    eeg_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {b: batch[b].to(device) for b in batch}\n",
    "            \n",
    "            eeg_embeds, _ = model(batch)\n",
    "            eeg_embeddings.append(eeg_embeds)\n",
    "    eeg_embeddings = torch.cat(eeg_embeddings)\n",
    "    return eeg_embeddings\n",
    "\n",
    "def compute_similarity(embeddings1, embeddings2):\n",
    "    return cosine_similarity(embeddings1.cpu().numpy(), embeddings2.cpu().numpy())\n",
    "\n",
    "def retrieve_closest(similarity_matrix, sentence_order, top_k=5):\n",
    "    closest_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_k]\n",
    "    closest_sentences = [[sentence_order[idx] for idx in row] for row in closest_indices]\n",
    "    return closest_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4677f6-a928-486d-a4f1-cd1d32371f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: print the closest matches for the first EEG embedding in the Test set\n",
    "# print(\"Top matches for the first EEG embedding in the Test set:\")\n",
    "# for idx in test_closest_matches[0]:\n",
    "#     print(f\"Text index: {idx}, Similarity: {test_similarity_matrix[0, idx]:.4f}\")\n",
    "#     text = tokenizer.decode(token_ids = test_ds[idx]['input_ids'], skip_special_tokens=True)\n",
    "#     print(f\"Matched text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee73173-05f3-425f-bd1d-fad36a3eefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(token_ids = test_ds[0]['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ff024-4226-4a71-ac66-5e62fd8ce9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import ConcatDataset\n",
    "# combined_dataset = ConcatDataset([train_ds, val_ds, test_ds])\n",
    "# combined_dataloader = DataLoader(combined_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "# combined_eeg_embeddings, combined_text_embeddings = embed_data(model, combined_dataloader)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_ds, batch_size = 32, shuffle = False, num_workers = 4)\n",
    "\n",
    "# unique_sentences = get_unique_sentences(dataset_dict)\n",
    "text_embeddings, sentence_order = embed_unique_sentences(model, all_words, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d4e435-4f5e-4ae5-ba8f-6db96bf82d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed EEG data\n",
    "train_eeg_embeddings = embed_eeg_data(model, train_dataloader)\n",
    "val_eeg_embeddings = embed_eeg_data(model, val_dataloader)\n",
    "test_eeg_embeddings = embed_eeg_data(model, test_dataloader)\n",
    "\n",
    "\n",
    "# Compute similarities\n",
    "train_similarity_matrix = compute_similarity(train_eeg_embeddings, text_embeddings)\n",
    "val_similarity_matrix = compute_similarity(val_eeg_embeddings, text_embeddings)\n",
    "test_similarity_matrix = compute_similarity(test_eeg_embeddings, text_embeddings)\n",
    "\n",
    "\n",
    "# Retrieve closest matches\n",
    "train_closest_matches = retrieve_closest(train_similarity_matrix, sentence_order)\n",
    "val_closest_matches = retrieve_closest(val_similarity_matrix, sentence_order)\n",
    "test_closest_matches = retrieve_closest(test_similarity_matrix, sentence_order)\n",
    "\n",
    "\n",
    "# Example: print the closest matches for the first EEG embedding in the validation set\n",
    "print(\"Top matches for the first EEG embedding in the validation set:\")\n",
    "for sentence in val_closest_matches[0]:\n",
    "    print(f\"Sentence: {sentence}, Similarity: {val_similarity_matrix[0, unique_sentences[sentence]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456c75e-c892-47b1-bcbc-d1fa7c22ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(val_ds[0]['input_ids'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8ca04-f297-4631-9977-3b920b3d3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top matches for the first EEG embedding in the train set:\")\n",
    "for sentence in train_closest_matches[1]:\n",
    "    print(f\"Sentence: {sentence}, Similarity: {train_similarity_matrix[1, unique_sentences[sentence]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86ddda-2d69-4483-b5b3-a1d5bb3b4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(train_ds[4]['input_ids'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa809f9-167a-428f-9480-6ae9b2b07ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_closest_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec05bc8-4d25-4cb6-bd39-ca1fdfd7e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EEGTextDatasetV3(Dataset):\n",
    "    def __init__(self, data_dict, subject_keys, tokenizer_name='bert-base-uncased', maxlen=15*500, mode='within', noise = False):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.maxlen = maxlen\n",
    "        self.data = []\n",
    "        self.subject_to_id = {}\n",
    "        self.mode = mode  # 'within', 'cross', or 'zero-shot'\n",
    "        self.noise = noise\n",
    "        \n",
    "        self.load_data(data_dict, subject_keys)\n",
    "\n",
    "    def load_data(self, data_dict, subject_keys):\n",
    "        for i, key in enumerate(subject_keys):\n",
    "            patient_data = data_dict[key]\n",
    "            sentences = np.array(patient_data['content'])\n",
    "            eeg_data = patient_data['eeg']\n",
    "            \n",
    "            if key not in self.subject_to_id:\n",
    "                self.subject_to_id[key] = len(self.subject_to_id)\n",
    "            subject_id = self.subject_to_id[key]\n",
    "            \n",
    "            mean, std = self.incremental_mean_std(eeg_data)\n",
    "\n",
    "            for sentence, eeg in zip(sentences, eeg_data):\n",
    "                if self.noise:\n",
    "                    eeg = torch.rand(eeg.shape)\n",
    "                eeg_processed, attention_mask = self.process_eeg(eeg, mean, std)\n",
    "                if eeg_processed is not None:\n",
    "                    self.data.append({\n",
    "                        'sentence': sentence,\n",
    "                        'eeg': eeg_processed,\n",
    "                        'eeg_attention_mask': attention_mask,\n",
    "                        'subject_id': subject_id\n",
    "                    })\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        tokenized = self.tokenizer(item['sentence'], return_tensors='pt', padding='max_length', truncation=True)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'][0],\n",
    "            'attention_mask': tokenized['attention_mask'][0],\n",
    "            'eeg': torch.nan_to_num(torch.tensor(item['eeg']), posinf=0, neginf=0).float(),\n",
    "            'eeg_attention_mask': torch.tensor(item['eeg_attention_mask']),\n",
    "            'subject_id': torch.tensor(item['subject_id'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_eeg(self, eeg_data, mean, std):\n",
    "        \"\"\"\n",
    "        Normalize EEG by computing total channel mean and std.\n",
    "        Right pad EEG with 0s to self.maxlen, throw error if eeg_data is longer than maxlen.\n",
    "        \"\"\"\n",
    "        if eeg_data.shape[0] < 100:\n",
    "            return None, None\n",
    "    \n",
    "        normalized_eeg = (eeg_data - mean) / std\n",
    "        \n",
    "        # Check if EEG data length exceeds maxlen\n",
    "        if normalized_eeg.shape[0] > self.maxlen:\n",
    "            print(f\"EEG data length {normalized_eeg.shape[0]} exceeds maxlen {self.maxlen}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = np.zeros((self.maxlen,))\n",
    "        attention_mask[:normalized_eeg.shape[0]] = 1\n",
    "        \n",
    "        # Right pad EEG data with zeros\n",
    "        padded_eeg = np.zeros((self.maxlen, normalized_eeg.shape[1]))\n",
    "        padded_eeg[:normalized_eeg.shape[0], :] = normalized_eeg\n",
    "        \n",
    "        return padded_eeg, attention_mask\n",
    "    \n",
    "    def incremental_mean_std(self, data_list):\n",
    "        \"\"\"\n",
    "        Calculate mean and standard deviation incrementally for a list of EEG data arrays.\n",
    "        \"\"\"\n",
    "        n_total = 0\n",
    "        mean = 0\n",
    "        M2 = 0\n",
    "        for data in data_list:\n",
    "            n = data.shape[0]\n",
    "            if n < 100:\n",
    "                continue\n",
    "        n_total += n\n",
    "        delta = data - mean\n",
    "        mean += np.nansum(delta, axis=0) / n_total\n",
    "        delta2 = data - mean\n",
    "        M2 += np.nansum(delta * delta2, axis=0)\n",
    "\n",
    "        variance = M2 / (n_total - 1)\n",
    "        std = np.sqrt(variance)\n",
    "        return mean, std\n",
    "\n",
    "def create_data_splits(data_dict, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    all_sentences = set()\n",
    "    for subject_data in data_dict.values():\n",
    "        all_sentences.update(subject_data['content'])\n",
    "    \n",
    "    # Select test (zero-shot) sentences\n",
    "    test_sentences = set(random.sample(all_sentences, int(len(all_sentences) * test_ratio)))\n",
    "    \n",
    "    train_val_data = {subject: {'content': [], 'eeg': []} for subject in data_dict}\n",
    "    test_data = {subject: {'content': [], 'eeg': []} for subject in data_dict}\n",
    "\n",
    "    for subject, subject_data in data_dict.items():\n",
    "        for sentence, eeg in zip(subject_data['content'], subject_data['eeg']):\n",
    "            if sentence in test_sentences:\n",
    "                test_data[subject]['content'].append(sentence)\n",
    "                test_data[subject]['eeg'].append(eeg)\n",
    "                # print(eeg)\n",
    "            else:\n",
    "                train_val_data[subject]['content'].append(sentence)\n",
    "                train_val_data[subject]['eeg'].append(eeg)\n",
    "    \n",
    "    # Split remaining data into train and validation\n",
    "    train_data = {subject: {'content': [], 'eeg': []} for subject in data_dict}\n",
    "    val_data = {subject: {'content': [], 'eeg': []} for subject in data_dict}\n",
    "\n",
    "    for subject, subject_data in train_val_data.items():\n",
    "        n = len(subject_data['content'])\n",
    "        train_idx = int(n * (train_ratio / (train_ratio + val_ratio)))\n",
    "        \n",
    "        train_data[subject]['content'] = subject_data['content'][:train_idx]\n",
    "        train_data[subject]['eeg'] = subject_data['eeg'][:train_idx]\n",
    "        \n",
    "        val_data[subject]['content'] = subject_data['content'][train_idx:]\n",
    "        val_data[subject]['eeg'] = subject_data['eeg'][train_idx:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def create_datasets(data_dict, tokenizer_name, maxlen, train_noise=False, val_noise=False, test_noise=False):\n",
    "    train_data, val_data, test_data = create_data_splits(data_dict)\n",
    "    \n",
    "    # # Within-subject datasets\n",
    "    # train_within = EEGTextDatasetV2(train_data, list(train_data.keys()), tokenizer_name, maxlen, mode='within')\n",
    "    # val_within = EEGTextDatasetV2(val_data, list(val_data.keys()), tokenizer_name, maxlen, mode='within')\n",
    "    \n",
    "    # Cross-subject dataset\n",
    "    train_data = {subject: {'content': train_data[subject]['content'],\n",
    "                          'eeg': train_data[subject]['eeg']}\n",
    "                for subject in train_data.keys()}\n",
    "    val_data = {subject: {'content': val_data[subject]['content'],\n",
    "                          'eeg':val_data[subject]['eeg']} for subject in val_data.keys()}\n",
    "    \n",
    "    train_cross = EEGTextDatasetV3(train_data, list(train_data.keys()), tokenizer_name, maxlen, mode='cross', noise = train_noise)\n",
    "    val_cross = EEGTextDatasetV3(val_data, list(val_data.keys()), tokenizer_name, maxlen, mode='cross', noise = val_noise)\n",
    "    \n",
    "    # Test dataset (zero-shot)\n",
    "    test = EEGTextDatasetV3(test_data, list(test_data.keys()), tokenizer_name, maxlen, mode='test', noise=test_noise)\n",
    "    \n",
    "    return train_cross, val_cross, test # train_within, val_within, \n",
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8955b4-4505-482e-b3c7-2a52369b0e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92522ad9-3bc6-4851-b281-5661ee8f7037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6c220-492c-4165-8b03-bbc48b298e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

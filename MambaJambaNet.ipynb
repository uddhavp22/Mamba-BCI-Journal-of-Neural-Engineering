{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "mount_file_id": "19zCAcX_GVMXXT28x3FLj_EWq6sunYRAa",
      "authorship_tag": "ABX9TyP391tdsRZnUdzJO99GhzHp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uddhavp22/Mamba-BCI-Journal-of-Neural-Engineering/blob/main/MambaJambaNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "5rmKySwAUdAb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X26i-64UNcio"
      },
      "outputs": [],
      "source": [
        "!pip install mne\n",
        "import mne\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch import tensor\n",
        "mne.set_log_level('ERROR')\n",
        "\n",
        "\n",
        "import scipy as sp\n",
        "!pip install torch_geometric\n",
        "import torch_geometric\n",
        "from torch_geometric import utils\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "!pip install mamba-ssm\n",
        "import torch\n",
        "from mamba_ssm import Mamba\n",
        "\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from mamba_ssm.modules.block import Block\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mamba Jamba Architecture"
      ],
      "metadata": {
        "id": "fw40k94NUgvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Mambablock(nn.Module):\n",
        "    def __init__(self, d_model, n_blocks, d_state=16, expand=2):\n",
        "        super(Mambablock, self).__init__()\n",
        "        self.n_blocks = n_blocks\n",
        "        self.d_model = d_model\n",
        "        config = {'d_state': d_state, 'expand': expand}\n",
        "\n",
        "        # Create Mamba instance with d_model as a keyword argument\n",
        "        self.four_mamba_blocks = nn.ModuleList([Block(d_model, partial(Mamba,**config), nn.Identity) for _ in range(self.n_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        count=0\n",
        "        for block in self.four_mamba_blocks:\n",
        "            x = block(x)[0]\n",
        "            print(count)\n",
        "        return x\n",
        "\n",
        "\n",
        "input_tensor = torch.randn(1, 1001, 1).to(\"cuda\")\n",
        "\n",
        "\n",
        "#num_channels = 22  # Define num_channels\n",
        "#num_layers = 4  # Define num_layers, choose as needed\n",
        "#im = 1 # Define dim, choose as needed\n",
        "model = Mambablock(d_model=1, n_blocks=4).to(\"cuda\")\n",
        "print(model(input_tensor).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X2C2d3lN3d2",
        "outputId": "ecd747fb-766d-4a76-eeeb-e998c8019826"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "torch.Size([1, 1001, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Mambablock(nn.Module):\n",
        "    def __init__(self, d_model, n_blocks, d_state=16, expand=2):\n",
        "        super(Mambablock, self).__init__()\n",
        "        self.n_blocks = n_blocks\n",
        "        self.d_model = d_model\n",
        "        config = {'d_state': d_state, 'expand': expand}\n",
        "\n",
        "        # Create Mamba instance with d_model as a keyword argument\n",
        "        self.mamba_blocks = nn.ModuleList([Block(d_model, partial(Mamba,**config), nn.Identity) for _ in range(self.n_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.mamba_blocks:\n",
        "            x = block(x)[0]\n",
        "        return x\n",
        "\n",
        "class EEGMamba(nn.Module):\n",
        "    def __init__(self, num_channels, num_blocks, dim, d_state=256, d_conv=4, expand=2,d_model=1):\n",
        "        super(EEGMamba, self).__init__()\n",
        "        # Pass dim as d_model to Mambablock\n",
        "        self.channel_models = nn.ModuleList([Mambablock(dim, num_blocks, d_state, expand) for _ in range(num_channels)])\n",
        "        #self.linear = nn.Linear(1001, d_state) if u want a linear transform after data\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        channel_outputs = []\n",
        "\n",
        "        # Iterate over each channel model and corresponding channel data\n",
        "        for i in range(x.size(1)):\n",
        "            channel_data = x[:, i, :, :]  # Extract data for the i-th channel\n",
        "\n",
        "\n",
        "            model = self.channel_models[i]\n",
        "            output = model(channel_data)\n",
        "            averaged_output = torch.mean(output, dim=1, keepdim=True)\n",
        "            channel_outputs.append(averaged_output)\n",
        "\n",
        "        # Stack the channel outputs along the correct dimension\n",
        "        combined_output = torch.stack(channel_outputs, dim=1)\n",
        "        return combined_output\n",
        "\n",
        "class MambaJambaNet(nn.Module):\n",
        "    def __init__(self, num_channels, num_layers, dim, d_state=16, d_conv=4, expand=2, kernel_size=2, num_classes=1):\n",
        "        super(MambaJambaNet, self).__init__()\n",
        "        self.mamba_model = EEGMamba(num_channels, num_layers, dim, d_state, d_conv, expand)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(num_channels, 46),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(46, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mamba_model(x)\n",
        "        print(x)\n",
        "        x = x.squeeze(2)  # making it (batch_size, num_channels, dim)\n",
        "        x = x.squeeze(2)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "input_tensor = torch.randn(1, 22, 1001, 1).to(\"cuda\")\n",
        "num_channels = 22  # Define num_channels\n",
        "num_layers = 4  # Define num_layers, choose as needed\n",
        "dim = 1 # Define dim, choose as needed\n",
        "model = MambaJambaNet(num_channels=22, num_layers=4, dim=1).to(\"cuda\")\n",
        "print(model(input_tensor).shape)\n",
        "\n",
        "#rn total parameters - 11,888"
      ],
      "metadata": {
        "id": "xhXt0lmVPMKv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "dcce6fcc-b152-4366-9c45-a2fc02e7467b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e98978bbcdd6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMambablock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMambablock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Number of parameters: {total_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUCPyk7wM9MW",
        "outputId": "841efa20-ca14-4c26-c249-22fab1766114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 11888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bCYCAm-l9bN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mamba2d"
      ],
      "metadata": {
        "id": "sCKVQROXjPyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mambablock(nn.Module):\n",
        "    def __init__(self, d_model, n_blocks, d_state=16, expand=2):\n",
        "        super(Mambablock, self).__init__()\n",
        "        self.n_blocks = n_blocks\n",
        "        self.d_model = d_model\n",
        "        config = {'d_state': d_state, 'expand': expand}\n",
        "\n",
        "        # Create Mamba instance with d_model as a keyword argument\n",
        "        self.four_mamba_blocks = nn.ModuleList([Block(d_model, partial(Mamba,**config), nn.Identity) for _ in range(self.n_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        count=0\n",
        "        for block in self.four_mamba_blocks:\n",
        "            x = block(x)[0]\n",
        "            print(count)\n",
        "        return x\n",
        "\n",
        "\n",
        "input_tensor = torch.randn(1, 22, 1001).to(\"cuda\")\n",
        "input_tensor = input_tensor.permute(0, 2, 1)\n",
        "\n",
        "#num_channels = 22  # Define num_channels\n",
        "#num_layers = 4  # Define num_layers, choose as needed\n",
        "#im = 1 # Define dim, choose as needed\n",
        "model = Mambablock(d_model=22, n_blocks=8).to(\"cuda\")\n",
        "print(model(input_tensor).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyeYDmhXjUSS",
        "outputId": "6912dc80-0336-409f-9d55-1654f3e62742"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "torch.Size([1, 1001, 22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "079JuG-KnaeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}